{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "TEXT_DATA_DIR = './writingPrompts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 texts.\n"
     ]
    }
   ],
   "source": [
    "# Data is split between a few different files\n",
    "texts = []         # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []        # list of label ids\n",
    "label_text = []    # list of label texts\n",
    "# Go through each directory\n",
    "for fname in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    label_id = len(labels_index)\n",
    "    fpath = os.path.join(TEXT_DATA_DIR, fname)\n",
    "    f = open(fpath, encoding='cp850')\n",
    "    t = f.read()\n",
    "    i = t.find('\\n\\n')  # skip header in file (starts with two newlines.)\n",
    "    if 0 < i:\n",
    "        t = t[i:]\n",
    "    texts.append(t)\n",
    "    f.close()\n",
    "    labels.append(label_id)\n",
    "    label_text.append(fname)\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.wp_source',\n",
       " 'test.wp_target',\n",
       " 'train.wp_source',\n",
       " 'train.wp_target',\n",
       " 'valid.wp_source',\n",
       " 'valid.wp_target']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14331433"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = []\n",
    "# Go through each text in turn\n",
    "for ii in range(len(texts)):\n",
    "    sentences = sent_tokenize(texts[ii])\n",
    "    sentences = [x for x in sentences if x != ['']]\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [x for x in words if x != ['']]\n",
    "        all_words.append(words)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Phrase Detection\n",
    "# Give some common terms that can be ignored in phrase detection\n",
    "# For example, 'state_of_affairs' will be detected because 'of' is provided here: \n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\", \"an\", \".\", \",\", \"!\", \"?\",]\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "phrases = Phrases(all_words, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "all_sentences = list(bigram[all_words])\n",
    "all_sentences[5673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser.load('./embedding/bigram.phr')\n",
    "all_sentences = list(bigram[all_words])\n",
    "all_sentences[5673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(all_words, \n",
    "                 min_count=2,   # Ignore words that appear less than this\n",
    "                 size=128,      # Dimensionality of word embeddings\n",
    "                 workers=2,     # Number of processors (parallelisation)\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=30)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./embedding/w2v_128.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379479"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
